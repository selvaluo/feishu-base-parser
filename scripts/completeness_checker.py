#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ€§æ ¡éªŒå™¨ (Completeness Checker)
====================================
åŠŸèƒ½ï¼šæ£€æŸ¥é£ä¹¦å¤šç»´è¡¨æ ¼ .base æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®å­—æ®µï¼Œ
      æ‰¾å‡ºè§£æå™¨å¯èƒ½é—æ¼çš„å­—æ®µï¼Œç”Ÿæˆæ ¡éªŒæŠ¥å‘Šã€‚
      
è¾“å‡ºï¼šå®Œæ•´æ€§æ ¡éªŒæŠ¥å‘Š.md
"""

import json
import base64
import gzip
import io
from collections import defaultdict

# ========== é…ç½® ==========
FILE_PATH = "ã€æ¼”ç¤ºã€‘æˆå“å¸ƒç®¡ç†ç³»ç»Ÿ.base"
OUTPUT_PATH = "å®Œæ•´æ€§æ ¡éªŒæŠ¥å‘Š.md"

# å·²çŸ¥çš„ã€å·²è¢«è§£æå™¨å¤„ç†çš„å­—æ®µï¼ˆæ ¹æ® generate_è‡ªåŠ¨åŒ–åœ°å›¾.py çš„é€»è¾‘ï¼‰
KNOWN_STEP_KEYS = {
    # é€šç”¨
    'type', 'id', 'data', 'stepTitle',
    # è§¦å‘å™¨
    'tableId', 'fields', 'triggerControlList', 'watchedFieldId', 'rule', 'startTime',
    'buttonType',  # ButtonTrigger
    # æŸ¥æ‰¾è®°å½•
    'recordInfo', 'fieldsMap', 'fieldIds', 'recordType', 'shouldProceedWithNoResults',
    # ä¿®æ”¹/æ–°å¢è®°å½•
    'recordList', 'updateFields', 'values', 'maxSetRecordNum',
    # æ¡ä»¶åˆ†æ”¯
    'condition', 'ifStepId', 'elseStepId', 'meetConditionStepId', 'notMeetConditionStepId',
    # å¾ªç¯
    'loopType', 'loopData', 'loopStartStepId', 'maxLoopCount', 'maxLoopTimes', 'loopMode', 'startChildStepId',
    # CustomAction
    'packId', 'formData', 'version', 'endpointId', 'resultTypeInfo', 'packType',
    # å…¶ä»–å¸¸è§å­—æ®µ
    'filterInfo', 'isEnabled', 'stepNum'
}

KNOWN_WORKFLOW_KEYS = {
    'id', 'base_id', 'trigger_name', 'creator', 'editor', 'status', 'delete_flag',
    'created_time', 'updated_time', 'source', 'access_mode', 'webhook_token',
    'biz_type', 'nodeSchema', 'WorkflowExtra'
}

KNOWN_DRAFT_KEYS = {
    'title', 'steps', 'version'
}


def decompress_content(compressed_content):
    """è§£å‹ gzip + base64 ç¼–ç çš„å†…å®¹"""
    try:
        if isinstance(compressed_content, str):
            compressed_bytes = base64.b64decode(compressed_content)
        else:
            return None
        with gzip.GzipFile(fileobj=io.BytesIO(compressed_bytes)) as gz:
            return json.loads(gz.read().decode('utf-8'))
    except Exception as e:
        print(f"è§£å‹å¤±è´¥: {e}")
        return None


def analyze_unknown_keys(data, known_keys, context=""):
    """åˆ†ææ•°æ®ä¸­çš„æœªçŸ¥é”®"""
    unknown = {}
    if isinstance(data, dict):
        for k, v in data.items():
            if k not in known_keys:
                unknown[k] = {
                    'context': context,
                    'value_type': type(v).__name__,
                    'sample': str(v)[:200] if v else "[ç©º]"
                }
    return unknown


def main():
    print("=" * 50)
    print("å®Œæ•´æ€§æ ¡éªŒå™¨")
    print("=" * 50)
    
    # è¯»å–æ–‡ä»¶
    print(f"\n[1/4] è¯»å–æ–‡ä»¶: {FILE_PATH}")
    try:
        with open(FILE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥é¡¶å±‚ç»“æ„
    print("[2/4] æ£€æŸ¥é¡¶å±‚æ•°æ®å—...")
    top_level_keys = set(data.keys())
    known_top_keys = {'gzipSnapshot', 'gzipExtraInfo', 'gzipBaseRole', 'gzipAccessConfig', 
                      'gzipDashboard', 'gzipAutomation', 'gzipAutomationButtonRule', 'sign'}
    unknown_top = top_level_keys - known_top_keys
    
    # è§£å‹è‡ªåŠ¨åŒ–æ•°æ®
    print("[3/4] è§£å‹å¹¶åˆ†æè‡ªåŠ¨åŒ–æ•°æ®...")
    workflows = decompress_content(data.get('gzipAutomation'))
    if not workflows or not isinstance(workflows, list):
        print("âŒ è‡ªåŠ¨åŒ–æ•°æ®è§£å‹å¤±è´¥")
        return
    
    # æ”¶é›†æ‰€æœ‰æœªçŸ¥å­—æ®µ
    all_unknown = defaultdict(list)
    step_type_fields = defaultdict(lambda: defaultdict(int))  # step_type -> {field: count}
    
    for wf in workflows:
        # æ£€æŸ¥å·¥ä½œæµçº§åˆ«
        wf_unknown = analyze_unknown_keys(wf, KNOWN_WORKFLOW_KEYS, f"å·¥ä½œæµ {wf.get('id', '?')}")
        for k, v in wf_unknown.items():
            all_unknown[f"å·¥ä½œæµçº§åˆ«.{k}"].append(v)
        
        # è§£æ Draft
        extra = wf.get('WorkflowExtra', {})
        draft_str = extra.get('Draft', '{}')
        try:
            draft = json.loads(draft_str) if isinstance(draft_str, str) else draft_str
        except:
            continue
        
        if not isinstance(draft, dict):
            continue
            
        # æ£€æŸ¥ Draft çº§åˆ«
        draft_unknown = analyze_unknown_keys(draft, KNOWN_DRAFT_KEYS, f"Draft")
        for k, v in draft_unknown.items():
            all_unknown[f"Draftçº§åˆ«.{k}"].append(v)
        
        # æ£€æŸ¥æ¯ä¸ªæ­¥éª¤
        for step in draft.get('steps', []):
            step_type = step.get('type', 'Unknown')
            step_data = step.get('data', {})
            
            # è®°å½•æ­¥éª¤çº§åˆ«çš„æœªçŸ¥å­—æ®µ
            step_unknown = analyze_unknown_keys(step, {'type', 'id', 'data', 'stepTitle'}, f"æ­¥éª¤ {step_type}")
            for k, v in step_unknown.items():
                all_unknown[f"æ­¥éª¤çº§åˆ«.{k}"].append(v)
            
            # è®°å½•æ­¥éª¤æ•°æ®ä¸­çš„æ‰€æœ‰å­—æ®µï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            for k in step_data.keys():
                step_type_fields[step_type][k] += 1
    
    # ç”ŸæˆæŠ¥å‘Š
    print("[4/4] ç”Ÿæˆæ ¡éªŒæŠ¥å‘Š...")
    
    # ç»Ÿè®¡æ•°æ®
    table_count = len(set(wf.get('base_id', '') for wf in workflows))
    workflow_count = len(workflows)
    unknown_count = sum(1 for fields in step_type_fields.values() 
                        for f in fields if f not in KNOWN_STEP_KEYS)
    
    # æ”¶é›†å…·ä½“é—®é¢˜
    problems = []
    
    # æ£€æŸ¥æœªçŸ¥æ­¥éª¤ç±»å‹å­—æ®µ
    for step_type, fields in step_type_fields.items():
        for field in fields:
            if field not in KNOWN_STEP_KEYS:
                problems.append({
                    'type': 'æœªè§£æçš„æ­¥éª¤å­—æ®µ',
                    'location': f'{step_type} ç±»å‹çš„æ­¥éª¤',
                    'detail': f'å­—æ®µ `{field}` æœªè¢«è§£æ',
                    'suggestion': f'å‘Šè¯‰ AIï¼š"{step_type} æ­¥éª¤ä¸­çš„ {field} å­—æ®µæ²¡æœ‰è¢«è§£æ"'
                })
    
    # ========== æ‰«æç”Ÿæˆçš„æ–‡æ¡£ï¼Œæ£€æŸ¥æœªç¿»è¯‘çš„ ID ==========
    import re
    import os
    
    # 0. æå–æºæ–‡ä»¶ä¸­æ‰€æœ‰çš„æœ‰æ•ˆ ID (ç”¨äºè¯Šæ–­)
    valid_ids = set()
    
    # æå–è¡¨ ID å’Œå­—æ®µ ID
    if isinstance(data, dict):
        extra = data.get('gzipExtraInfo', {})
        if isinstance(extra, str): # å¦‚æœè¿˜æ²¡è§£å‹
             extra = decompress_content(extra)
        
        if isinstance(extra, dict):
            tables = extra.get('tables', [])
            for tbl in tables:
                tid = tbl.get('tableId')
                if tid: valid_ids.add(tid)
                
                for fld in tbl.get('fields', []):
                    fid = fld.get('fieldId')
                    if fid: valid_ids.add(fid)
    
    doc_files = [
        "å…¨é‡å­—æ®µè¡¨.md",
        "å­—æ®µå…³è”å…³ç³»å›¾.md",
        "è‡ªåŠ¨åŒ–å·¥ä½œæµ.md"
    ]
    
    # åŒ¹é…æ¨¡å¼ï¼š(æ­£åˆ™, ç±»å‹åç§°, æ˜¯å¦æ•…æ„æ˜¾ç¤º)
    # [æœªçŸ¥å­—æ®µ:fldXXX]
    id_patterns = [
        (r'\[æœªçŸ¥(?:å­—æ®µ|è¡¨|é€‰é¡¹|å¼•ç”¨)[^:\]]*:([^\]]+)\]', 'æ˜¾å¼æœªçŸ¥é¡¹', 'æœªè§£æ'),
        (r'\[å·²åˆ é™¤çš„(?:å­—æ®µ|è¡¨)[^:\]]*:([^\]]+)\]', 'å·²åˆ é™¤å¼•ç”¨', 'æœªè§£æ'),
        (r'\[æ­¥éª¤\d+çš„(?:å­—æ®µ|formula|ç»“æœ)\]', 'æ¨¡ç³Šå¼•ç”¨', 'å¯è¯»æ€§å·®'),
        (r'\[æ­¥éª¤\d+çš„å¾ªç¯å½“å‰è®°å½•\]', 'æ¨¡ç³Šå¾ªç¯', 'å¯è¯»æ€§å·®'),
        (r'default_url":\s*"{å¼•ç”¨}"', 'æ¨¡ç³ŠåŠ¨ä½œé…ç½®', 'ä¿¡æ¯ä¸¢å¤±'),
        (r'\b(is|isNot|contains|doesNotContain|isEmpty|isNotEmpty)\b', 'æœªç¿»è¯‘æ“ä½œç¬¦', 'è‹±æ–‡æ®‹ç•™')
    ]
    
    untranslated_items = []
    
    for doc_path in doc_files:
        if not os.path.exists(doc_path):
            continue
        
        with open(doc_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        doc_name = os.path.basename(doc_path)
        
        for pattern, issue_type, category in id_patterns:
            for match in re.finditer(pattern, content):
                match_text = match.group(0) # å®Œæ•´æ ‡ç­¾
                # å¯¹äºæŸäº›æ­£åˆ™ï¼Œå¯èƒ½æ²¡æœ‰ group(1)
                match_id = match.group(1) if match.lastindex and match.lastindex >= 1 else match_text
                match_start = match.start()
                
                # æ‰¾åˆ°è¡Œå·
                line_num = content[:match_start].count('\n') + 1

                # è·å–è¯¥è¡Œå†…å®¹
                line_start = content.rfind('\n', 0, match_start) + 1
                line_end = content.find('\n', match_start)
                if line_end == -1: line_end = len(content)
                line_content = content[line_start:line_end]

                # å°è¯•è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ (æ‰€å±è¡¨å / å­—æ®µå)
                context_info = "æœªçŸ¥ä½ç½®"
                
                # 1. å‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„äºŒçº§æ ‡é¢˜ (## è¡¨å)
                header_match = None
                for m in re.finditer(r'^##\s+(.*?)$', content[:match_start], re.MULTILINE):
                    header_match = m
                
                table_name = header_match.group(1).strip() if header_match else "æœªçŸ¥è¡¨"
                
                # 2. å°è¯•ä»å½“å‰è¡Œæå–ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ (å­—æ®µå)
                field_name = "æœªçŸ¥è¡Œ"
                row_match = re.match(r'^\|?\s*\*{0,2}(.*?)\*{0,2}\s*\|', line_content.strip())
                if row_match:
                    field_name = row_match.group(1).strip()
                
                context_str = f"è¡¨: {table_name} / è¡Œ: {field_name}"
                
                # è¯Šæ–­åŸå› 
                diagnosis = ""
                action = ""
                
                if category == 'æœªè§£æ':
                    if match_id in valid_ids:
                        reason = "è§£æå™¨ç¼ºé™·"
                        diagnosis = f"ID `{match_id}` å­˜åœ¨äºæºæ•°æ®ä¸­ï¼Œä½†è§£æå™¨æœªèƒ½è¯†åˆ«ã€‚"
                        action = "å»ºè®®ï¼šè¯·æ£€æŸ¥ç”Ÿæˆè„šæœ¬çš„ ID æ˜ å°„é€»è¾‘ã€‚"
                        severity = "ğŸ”´ é«˜ (å¯èƒ½æ˜¯ Bug)"
                    else:
                        reason = "æ•°æ®ç¼ºå¤±"
                        diagnosis = f"ID `{match_id}` åœ¨æºæ•°æ®ä¸­ä¸å­˜åœ¨ã€‚"
                        action = (
                            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
                            "  1. æ‰“å¼€é£ä¹¦å¤šç»´è¡¨æ ¼\n"
                            f"  2. å®šä½åˆ° **{table_name}**\n"
                            f"  3. æ‰¾åˆ° **{field_name}** (æˆ–å¯¹åº”è‡ªåŠ¨åŒ–æµç¨‹)\n"
                            "  4. æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾ç¤ºä¸º **çº¢è‰²é”™è¯¯** æˆ– **å·²åˆ é™¤** çš„å­—æ®µå¼•ç”¨\n"
                            "  5. å¦‚æœè¯¥å­—æ®µç¡®å®å­˜åœ¨ä¸”æ­£å¸¸ï¼Œè¯·**æˆªå›¾**è¯¥å­—æ®µçš„é…ç½®å‘é€ç»™ AI"
                        )
                        severity = "ğŸŸ¡ ä¸­ (å¯èƒ½æ˜¯å·²åˆ é™¤å­—æ®µ)"
                else:
                    reason = issue_type
                    diagnosis = f"å‘ç° {issue_type}: `{match_text}`"
                    action = "è¿™æ˜¯è„šæœ¬ç”Ÿæˆé€»è¾‘ä¸å¤Ÿå®Œå–„å¯¼è‡´çš„ï¼Œè¯·å‘ŠçŸ¥ AI ä¼˜åŒ–ç›¸å…³è§£æå‡½æ•°ã€‚"
                    severity = "ğŸ”µ ä½ (å¯è¯»æ€§é—®é¢˜)"

                untranslated_items.append({
                    'doc': doc_name,
                    'line': line_num,
                    'text': match_text,
                    'id': match_id,
                    'context': context_str,
                    'reason': reason,
                    'diagnosis': diagnosis,
                    'action': action,
                    'severity': severity
                })
    
    lines = []
    lines.append("# å®Œæ•´æ€§æ ¡éªŒæŠ¥å‘Š\n")
    lines.append(f"> ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append("---\n")
    
    # æ ¡éªŒç»“æœæ‘˜è¦
    lines.append("## ğŸ“Š æ ¡éªŒç»“æœ\n")
    lines.append("| é¡¹ç›® | ç»“æœ |")
    lines.append("|------|------|")
    lines.append(f"| å·¥ä½œæµè§£æ | âœ… {workflow_count} ä¸ªå·¥ä½œæµå·²è§£æ |")
    
    if unknown_count == 0:
        lines.append("| å­—æ®µè¦†ç›–ç‡ | âœ… 100% å…¨éƒ¨è¦†ç›– |")
    else:
        coverage = 100 - (unknown_count / max(1, sum(len(f) for f in step_type_fields.values())) * 100)
        lines.append(f"| å­—æ®µè¦†ç›–ç‡ | âš ï¸ {coverage:.1f}% (æœ‰ {unknown_count} ä¸ªå­—æ®µæœªè§£æ) |")
    
    # ç¿»è¯‘è¦†ç›–ç‡
    if len(untranslated_items) == 0:
        lines.append("| IDç¿»è¯‘ | âœ… 100% å·²ç¿»è¯‘ |")
    else:
        lines.append(f"| IDç¿»è¯‘ | âš ï¸ å‘ç° {len(untranslated_items)} ä¸ªæœªç¿»è¯‘ID |")
    
    lines.append("")
    
    # é—®é¢˜åˆ—è¡¨
    if problems:
        lines.append("---\n")
        lines.append("## âš ï¸ å‘ç°çš„é—®é¢˜ (éœ€äººå·¥ä»‹å…¥)\n")
        
        for i, p in enumerate(problems[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            lines.append(f"### é—®é¢˜ {i}: {p['type']}\n")
            lines.append(f"- **ä½ç½®**: {p['location']}")
            lines.append(f"- **è¯¦æƒ…**: {p['detail']}")
            lines.append(f"- **å¦‚ä½•ä¿®å¤**: {p['suggestion']}\n")
        
        if len(problems) > 5:
            lines.append(f"\n*è¿˜æœ‰ {len(problems) - 5} ä¸ªç±»ä¼¼é—®é¢˜...*\n")
    
    # ç”Ÿæˆé—®é¢˜åˆ—è¡¨
    if untranslated_items:
        lines.append("---\n")
        lines.append("## âš ï¸ å‘ç°çš„é—®é¢˜ (éœ€äººå·¥ä»‹å…¥)\n")
        
        for i, item in enumerate(untranslated_items[:10], 1):
            # æ„å»ºå¯ç‚¹å‡»é“¾æ¥ (VS Code å‹å¥½æ ¼å¼)
            file_link = f"[{item['doc']}:{item['line']}](./{item['doc']}#L{item['line']})"
            
            lines.append(f"### é—®é¢˜ {i}: {item['reason']}\n")
            lines.append(f"- **é”™è¯¯ä½ç½®**: {file_link}")
            lines.append(f"- **ç²¾ç¡®å®šä½**: {item['context']}")
            lines.append(f"- **æœªè§£æå†…å®¹**: `{item['text']}`")
            lines.append(f"- **è¯Šæ–­ç»“æœ**: {item['diagnosis']}")
            lines.append(f"- **å»ºè®®æ“ä½œ**: \n{item['action']}\n")
            
        if len(untranslated_items) > 10:
             lines.append(f"\n*è¿˜æœ‰ {len(untranslated_items) - 10} ä¸ªç±»ä¼¼é—®é¢˜...*\n")

    else:
        # å¦‚æœæ²¡æœ‰é—®é¢˜
        lines.append("---\n")
        lines.append("## âœ… è§£æå®Œæˆ\n")
        lines.append("æ‰€æœ‰å†…å®¹å‡å·²æˆåŠŸè§£æï¼Œæ— éœ€é¢å¤–å¤„ç†ã€‚\n")
        
    # ä½¿ç”¨è¯´æ˜
    lines.append("---\n")
    lines.append("## ğŸ’¬ å¦‚æœæ‚¨å‘ç°å…¶ä»–é—®é¢˜\n")
    lines.append("åœ¨é˜…è¯»ç”Ÿæˆçš„æ–‡æ¡£æ—¶ï¼Œå¦‚æœçœ‹åˆ°ä»¥ä¸‹æƒ…å†µï¼š\n")
    lines.append("- æ˜¾ç¤ºä¸º `fldXXX` æˆ– `tblXXX` æ ¼å¼çš„å†…å®¹")
    lines.append("- æ˜¾ç¤ºä¸º `æœªçŸ¥ç±»å‹(æ•°å­—)` çš„å­—æ®µç±»å‹")
    lines.append("- æ˜¾ç¤ºä¸ºè‹±æ–‡çš„æ“ä½œæˆ–å­—æ®µ\n")
    lines.append("**è¯·ç›´æ¥å‘Šè¯‰ AI** é—®é¢˜å‡ºç°çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼š\n")
    lines.append('> "è‡ªåŠ¨åŒ–å·¥ä½œæµç¬¬ XX è¡Œæœ‰ä¸ªå­—æ®µæ˜¾ç¤ºä¸ºåŸå§‹ IDï¼Œå¸®æˆ‘ç¿»è¯‘ä¸€ä¸‹"\n')
    lines.append("AI ä¼šè‡ªåŠ¨ä¿®å¤å¹¶é‡æ–°ç”Ÿæˆæ–‡æ¡£ã€‚\n")
    
    # å†™å…¥æ–‡ä»¶
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"\nâœ… æ ¡éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {OUTPUT_PATH}")
    print("=" * 50)
    
    if untranslated_items:
        print(f"âš ï¸ å‘ç° {len(untranslated_items)} ä¸ªéœ€è¦äººå·¥ä»‹å…¥çš„é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š")
    else:
        print("âœ… æ‰€æœ‰å­—æ®µå‡å·²è¢«è§£æå™¨è¦†ç›–")


if __name__ == "__main__":
    main()
